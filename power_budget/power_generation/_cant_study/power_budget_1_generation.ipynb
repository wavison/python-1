{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "869283d0",
   "metadata": {},
   "source": [
    "# Step 1: Satellite Geometry\n",
    "\n",
    "1_geometry\n",
    "\n",
    "\n",
    "\n",
    "pip install matplotlib\n",
    "\n",
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92352a02",
   "metadata": {},
   "source": [
    "# Step 2: Orbits\n",
    "\n",
    "## Step 2.1\n",
    "\n",
    "## Description\n",
    "Collecting the different orbits to consider, these might be different altitudes and different LTANs\n",
    "\n",
    "## Inputs\n",
    "* SCENARIO=SSO_TO_KEPLERIAN_CONVERSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5468f316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing orbit_500_ltan_10_input.txt -> orbit_500_ltan_10_output.txt...\n",
      "  ✅ orbit_500_ltan_10_input.txt processed successfully.\n",
      "Processing orbit_500_ltan_12_input.txt -> orbit_500_ltan_12_output.txt...\n",
      "  ✅ orbit_500_ltan_12_input.txt processed successfully.\n",
      "Processing orbit_500_ltan_15_input.txt -> orbit_500_ltan_15_output.txt...\n",
      "  ✅ orbit_500_ltan_15_input.txt processed successfully.\n",
      "Processing orbit_500_ltan_9_input.txt -> orbit_500_ltan_9_output.txt...\n",
      "  ✅ orbit_500_ltan_9_input.txt processed successfully.\n",
      "All files processed. Check processing_log.txt for details.\n"
     ]
    }
   ],
   "source": [
    "# This script processes all input files in the \"2_orbit_inputs\" folder,\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Paths\n",
    "input_folder = \"2_orbit_inputs\"\n",
    "jar_file = \"technicalsupport.jar\"\n",
    "log_file = \"processing_log.txt\"\n",
    "\n",
    "# Find all .txt input files in the subfolder\n",
    "input_files = [f for f in os.listdir(input_folder) if f.endswith(\"_input.txt\")]\n",
    "\n",
    "# Prepare the log\n",
    "with open(log_file, \"w\") as log:\n",
    "    log.write(f\"Processing started at {datetime.now()}\\n\")\n",
    "    log.write(f\"Found {len(input_files)} input files in '{input_folder}'.\\n\\n\")\n",
    "\n",
    "    # Loop through each input file\n",
    "    for input_file in input_files:\n",
    "        input_path = os.path.join(input_folder, input_file)\n",
    "        output_file = input_file.replace(\"_input\", \"_output\")\n",
    "        output_path = os.path.join(input_folder, output_file)\n",
    "\n",
    "        log.write(f\"Processing: {input_file} -> {output_file}\\n\")\n",
    "        print(f\"Processing {input_file} -> {output_file}...\")\n",
    "\n",
    "        # Build the Java command\n",
    "        command = [\n",
    "            \"java\",\n",
    "            f\"-DINPUT={input_path}\",\n",
    "            f\"-DOUTPUT={output_path}\",\n",
    "            \"-jar\",\n",
    "            jar_file\n",
    "        ]\n",
    "\n",
    "        # Run the command\n",
    "        result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "        # Log results\n",
    "        if result.returncode == 0:\n",
    "            log.write(f\"  SUCCESS at {datetime.now()}\\n\\n\")\n",
    "            print(f\"  ✅ {input_file} processed successfully.\")\n",
    "        else:\n",
    "            log.write(f\"  ERROR at {datetime.now()}:\\n{result.stderr}\\n\\n\")\n",
    "            print(f\"  ❌ Error processing {input_file}. See log for details.\")\n",
    "\n",
    "    log.write(f\"Processing finished at {datetime.now()}\\n\")\n",
    "\n",
    "print(\"All files processed. Check processing_log.txt for details.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8249e6",
   "metadata": {},
   "source": [
    "## Step 2.2\n",
    "\n",
    "What it does:\n",
    "Reads each file in orbits/ ending with _output.txt.\n",
    "\n",
    "Splits every line on = into key and value.\n",
    "\n",
    "Builds a wide table:\n",
    "\n",
    "Row = parameter name (DATE, SEMIMAJOR_AXIS_MET, etc.)\n",
    "\n",
    "Column = each file’s name (without .txt).\n",
    "\n",
    "Outputs aggregated_outputs.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94fdc45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Aggregated key=value data saved to aggregated_orbit_outputs.csv\n"
     ]
    }
   ],
   "source": [
    "# Aggregating power orbit data from operational outputs\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder containing the output files\n",
    "input_folder = \"2_orbit_inputs\"\n",
    "output_csv = \"aggregated_orbit_outputs.csv\"\n",
    "\n",
    "# Find all *_output.txt files\n",
    "output_files = [f for f in os.listdir(input_folder) if f.endswith(\"_output.txt\")]\n",
    "\n",
    "# Dictionary to hold data\n",
    "data = {}\n",
    "\n",
    "for file in output_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Parse key=value pairs\n",
    "    values = {}\n",
    "    for line in lines:\n",
    "        if \"=\" in line:\n",
    "            key, val = line.strip().split(\"=\", 1)\n",
    "            values[key] = val\n",
    "\n",
    "    # Add to dictionary with filename as column\n",
    "    data[os.path.splitext(file)[0]] = values\n",
    "\n",
    "# Convert to DataFrame (keys as rows, files as columns)\n",
    "df = pd.DataFrame(data)\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={\"index\": \"Parameter\"}, inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Aggregated key=value data saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b942b",
   "metadata": {},
   "source": [
    "# Step 3: Power Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f878edc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 28 files in 4_generated_docs\n"
     ]
    }
   ],
   "source": [
    "# Generate operating input files from a template and CSV data\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "template_path = \"operating_template_input.txt\"\n",
    "csv_path = \"operational_inputs.csv\"\n",
    "output_dir = \"4_generated_docs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read template\n",
    "with open(template_path, 'r') as f:\n",
    "    template_content = f.read()\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Pivot so each scenario is a column\n",
    "df_pivot = df.set_index('Parameter')\n",
    "scenarios = df_pivot.columns.tolist()\n",
    "\n",
    "# Fields to populate\n",
    "fields = [\n",
    "    \"DATE\", \"SEMIMAJOR_AXIS_MET\", \"ECCENTRICITY\", \"INCLINATION_DEG\",\n",
    "    \"RAAN_DEG\", \"W_DEG\", \"TA_DEG\", \"ATTITUDE_MODE\", \"PRIMARY_VECTOR\",\n",
    "    \"SECONDARY_VECTOR\", \"ABSOLUTE_PATH_TO_FILE\", \"SPAN_DAYS\",\n",
    "    \"PRIMARY_TARGET\", \"SECONDARY_TARGET\"\n",
    "]\n",
    "\n",
    "# Generate files\n",
    "for scenario in scenarios:\n",
    "    scenario_data = df_pivot[scenario].to_dict()\n",
    "    content = template_content\n",
    "    \n",
    "    # Extract primary/secondary target (or use defaults)\n",
    "    primary_target = scenario_data.get(\"PRIMARY_TARGET\", \"VELOCITY\") or \"VELOCITY\"\n",
    "    secondary_target = scenario_data.get(\"SECONDARY_TARGET\", \"NADIR\") or \"NADIR\"\n",
    "    \n",
    "    # Replace fields with values\n",
    "    for field in fields:\n",
    "        value = scenario_data.get(field, \"\")\n",
    "        if field == \"PRIMARY_TARGET\":\n",
    "            value = primary_target\n",
    "        elif field == \"SECONDARY_TARGET\":\n",
    "            value = secondary_target\n",
    "        content = content.replace(f\"{field}=\", f\"{field}={value}\")\n",
    "    \n",
    "    # Auto-generate min/max power CSV paths\n",
    "    min_power = f\"./min_{scenario}_{primary_target}_{secondary_target}.csv\"\n",
    "    max_power = f\"./max_{scenario}_{primary_target}_{secondary_target}.csv\"\n",
    "    content = content.replace(\"MINIMUM_ORBIT_POWER_CSV=\", f\"MINIMUM_ORBIT_POWER_CSV={min_power}\")\n",
    "    content = content.replace(\"MAXIMUM_ORBIT_POWER_CSV=\", f\"MAXIMUM_ORBIT_POWER_CSV={max_power}\")\n",
    "    \n",
    "    # Clean scenario name: remove trailing .digit\n",
    "    clean_scenario = re.sub(r'\\.\\d+$', '', scenario)\n",
    "    \n",
    "    # Build filename with primary/secondary target\n",
    "    filename = os.path.join(output_dir, f\"{clean_scenario}_{primary_target}_{secondary_target}_input.txt\")\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(f\"Generated {len(scenarios)} files in {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b8308",
   "metadata": {},
   "source": [
    "# Step 5\n",
    "\n",
    "\n",
    "## Description\n",
    "Take the input files per orientation and produce the power output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b47bf8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing orbit_500_ltan_10_output_+10_VELOCITY_NADIR_input.txt -> orbit_500_ltan_10_output_+10_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_10_output_+10_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_10_output_+15_VELOCITY_NADIR_input.txt -> orbit_500_ltan_10_output_+15_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_10_output_+15_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_10_output_+20_VELOCITY_NADIR_input.txt -> orbit_500_ltan_10_output_+20_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_10_output_+20_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_10_output_+25_VELOCITY_NADIR_input.txt -> orbit_500_ltan_10_output_+25_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_10_output_+25_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_10_output_+30_VELOCITY_NADIR_input.txt -> orbit_500_ltan_10_output_+30_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_10_output_+30_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_10_output_-10_VELOCITY_NADIR_input.txt -> orbit_500_ltan_10_output_-10_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_10_output_-10_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_10_output_0_VELOCITY_NADIR_input.txt -> orbit_500_ltan_10_output_0_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_10_output_0_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_12_output_+10_VELOCITY_NADIR_input.txt -> orbit_500_ltan_12_output_+10_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_12_output_+10_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_12_output_+15_VELOCITY_NADIR_input.txt -> orbit_500_ltan_12_output_+15_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_12_output_+15_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_12_output_+20_VELOCITY_NADIR_input.txt -> orbit_500_ltan_12_output_+20_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_12_output_+20_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_12_output_+25_VELOCITY_NADIR_input.txt -> orbit_500_ltan_12_output_+25_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_12_output_+25_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_12_output_+30_VELOCITY_NADIR_input.txt -> orbit_500_ltan_12_output_+30_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_12_output_+30_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_12_output_-10_VELOCITY_NADIR_input.txt -> orbit_500_ltan_12_output_-10_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_12_output_-10_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_12_output_0_VELOCITY_NADIR_input.txt -> orbit_500_ltan_12_output_0_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_12_output_0_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_15_output_+10_VELOCITY_NADIR_input.txt -> orbit_500_ltan_15_output_+10_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_15_output_+10_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_15_output_+15_VELOCITY_NADIR_input.txt -> orbit_500_ltan_15_output_+15_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_15_output_+15_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_15_output_+20_VELOCITY_NADIR_input.txt -> orbit_500_ltan_15_output_+20_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_15_output_+20_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_15_output_+25_VELOCITY_NADIR_input.txt -> orbit_500_ltan_15_output_+25_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_15_output_+25_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_15_output_+30_VELOCITY_NADIR_input.txt -> orbit_500_ltan_15_output_+30_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_15_output_+30_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_15_output_-10_VELOCITY_NADIR_input.txt -> orbit_500_ltan_15_output_-10_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_15_output_-10_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_15_output_0_VELOCITY_NADIR_input.txt -> orbit_500_ltan_15_output_0_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_15_output_0_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_9_output_+10_VELOCITY_NADIR_input.txt -> orbit_500_ltan_9_output_+10_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_9_output_+10_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_9_output_+15_VELOCITY_NADIR_input.txt -> orbit_500_ltan_9_output_+15_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_9_output_+15_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_9_output_+20_VELOCITY_NADIR_input.txt -> orbit_500_ltan_9_output_+20_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_9_output_+20_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_9_output_+25_VELOCITY_NADIR_input.txt -> orbit_500_ltan_9_output_+25_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_9_output_+25_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_9_output_+30_VELOCITY_NADIR_input.txt -> orbit_500_ltan_9_output_+30_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_9_output_+30_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_9_output_-10_VELOCITY_NADIR_input.txt -> orbit_500_ltan_9_output_-10_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_9_output_-10_VELOCITY_NADIR_output.txt\n",
      "Processing orbit_500_ltan_9_output_0_VELOCITY_NADIR_input.txt -> orbit_500_ltan_9_output_0_VELOCITY_NADIR_output.txt...\n",
      "  ✅ Successfully generated orbit_500_ltan_9_output_0_VELOCITY_NADIR_output.txt\n",
      "All files processed. Outputs saved in 'operational_outputs'.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Folders and paths\n",
    "input_folder = \"4_generated_docs\"\n",
    "output_folder = \"5_operational_outputs\"\n",
    "jar_file = \"technicalsupport.jar\"\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get all input files ending with _input.txt\n",
    "input_files = [f for f in os.listdir(input_folder) if f.endswith(\"_input.txt\")]\n",
    "\n",
    "for input_file in input_files:\n",
    "    input_path = os.path.join(input_folder, input_file)\n",
    "    output_file = input_file.replace(\"_input.txt\", \"_output.txt\")\n",
    "    output_path = os.path.join(output_folder, output_file)\n",
    "    \n",
    "    # Build the Java command\n",
    "    command = [\n",
    "        \"java\",\n",
    "        f\"-DINPUT={input_path}\",\n",
    "        f\"-DOUTPUT={output_path}\",\n",
    "        \"-jar\",\n",
    "        jar_file\n",
    "    ]\n",
    "    \n",
    "    print(f\"Processing {input_file} -> {output_file}...\")\n",
    "    \n",
    "    # Run the command\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "    \n",
    "    # Check result\n",
    "    if result.returncode == 0:\n",
    "        print(f\"  ✅ Successfully generated {output_file}\")\n",
    "    else:\n",
    "        print(f\"  ❌ Error processing {input_file}: {result.stderr}\")\n",
    "\n",
    "print(\"All files processed. Outputs saved in 'operational_outputs'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378f1fb",
   "metadata": {},
   "source": [
    "# Step 6\n",
    "\n",
    "# Description\n",
    "Produce the charts that show power produced for the different orbits.\n",
    "What will be interesting:\n",
    "1. different operating modes\n",
    "2. different orbits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7696692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated table saved to aggregated_power_orbit_table.csv\n"
     ]
    }
   ],
   "source": [
    "# Aggregating power orbit data from operational outputs\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Config\n",
    "input_folder = \"5_operational_outputs\"\n",
    "output_csv = \"aggregated_power_orbit_table.csv\"\n",
    "\n",
    "# Files\n",
    "files = [f for f in os.listdir(input_folder) if f.endswith(\"_output.txt\")]\n",
    "\n",
    "# Ordered metrics for final table\n",
    "ordered_metrics = [\n",
    "    \"ECLIPSE ORBIT PERCENTAGE [%]\",\n",
    "    \"SUNLIGHT DURATION [s]\",\n",
    "    \"SUNLIGHT ORBIT PERCENTAGE [%]\",\n",
    "    \"AVERAGE POWER PER SUNLIGHT [W]\",\n",
    "    \"AVERAGE POWER PER ORBIT [W]\",\n",
    "    \"AVERAGE POWER PER SUNLIGHT [W],PERCENTAGE OF ORBITS [%]\",\n",
    "    \"AVERAGE POWER PER ORBIT [W],PERCENTAGE OF ORBITS [%]\"\n",
    "]\n",
    "\n",
    "data = {}\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    values = {}\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        if line in ordered_metrics[:5]:  # metrics with average/min/max\n",
    "            # Find average value\n",
    "            j = i + 1\n",
    "            while j < len(lines) and \"AVERAGE\" not in lines[j]:\n",
    "                j += 1\n",
    "            if j < len(lines):\n",
    "                avg_match = re.search(r\"AVERAGE\\s*=\\s*([-+]?[0-9]*\\.?[0-9]+)\", lines[j])\n",
    "                if avg_match:\n",
    "                    values[line] = round(float(avg_match.group(1)), 1)\n",
    "            i = j + 1\n",
    "        elif line in ordered_metrics[5:]:  # table metrics\n",
    "            # Get the highest percentage (≈99%)\n",
    "            j = i + 1\n",
    "            max_percentage_val = None\n",
    "            while j < len(lines) and \",\" in lines[j]:\n",
    "                val, perc = lines[j].split(\",\")\n",
    "                if float(perc) >= 99:\n",
    "                    max_percentage_val = round(float(val), 1)\n",
    "                    break\n",
    "                j += 1\n",
    "            if max_percentage_val is not None:\n",
    "                values[line] = max_percentage_val\n",
    "            i = j + 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    data[file] = values\n",
    "\n",
    "# Create DataFrame with files as columns and metrics as rows\n",
    "df = pd.DataFrame(data)\n",
    "df = df.reindex(ordered_metrics)  # enforce row order\n",
    "\n",
    "# Save CSV\n",
    "df.to_csv(output_csv)\n",
    "print(f\"Aggregated table saved to {output_csv}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18001655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charts saved in 6_analysis_charts\n"
     ]
    }
   ],
   "source": [
    "# Generate bar charts for average power per orbit\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "\n",
    "# --- Config ---\n",
    "input_csv = \"aggregated_power_orbit_table.csv\"\n",
    "output_folder = \"6_analysis_charts\"\n",
    "metric = \"AVERAGE POWER PER ORBIT [W]\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# --- Load Data ---\n",
    "df = pd.read_csv(input_csv, index_col=0)\n",
    "power_row = df.loc[metric]  # Extract the row with our metric\n",
    "\n",
    "# --- Parse filename components ---\n",
    "data = []\n",
    "for filename, value in power_row.items():\n",
    "    alt_match = re.search(r'orbit_(\\d+)', filename)\n",
    "    ltan_match = re.search(r'ltan_(\\d+)', filename)\n",
    "    mode_match = re.search(r'_(VELOCITY|SUN)_(NADIR|SUN|GS)', filename)\n",
    "\n",
    "    altitude = int(alt_match.group(1)) if alt_match else None\n",
    "    ltan = int(ltan_match.group(1)) if ltan_match else None\n",
    "    mode = f\"{mode_match.group(1)}-{mode_match.group(2)}\" if mode_match else \"UNKNOWN\"\n",
    "\n",
    "    data.append({\n",
    "        \"Filename\": filename,\n",
    "        \"Mode\": mode,\n",
    "        \"Altitude\": altitude,\n",
    "        \"LTAN\": ltan,\n",
    "        \"Power\": float(value)\n",
    "    })\n",
    "\n",
    "df_parsed = pd.DataFrame(data)\n",
    "\n",
    "# --- Generate one chart per Mode ---\n",
    "for mode in df_parsed[\"Mode\"].unique():\n",
    "    subset = df_parsed[df_parsed[\"Mode\"] == mode].sort_values(by=[\"Altitude\", \"LTAN\"])\n",
    "    \n",
    "    labels = [f\"Alt {row['Altitude']}\\nLTAN {row['LTAN']}\" for _, row in subset.iterrows()]\n",
    "    values = subset[\"Power\"].values\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(labels, values, color=\"skyblue\")\n",
    "    plt.title(f\"Average Power per Orbit [W] - {mode}\")\n",
    "    plt.ylabel(\"Power [W]\")\n",
    "    plt.xlabel(\"Scenario (Altitude & LTAN)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save chart\n",
    "    chart_path = os.path.join(output_folder, f\"{mode}_bar_chart.png\")\n",
    "    plt.savefig(chart_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "print(f\"Charts saved in {output_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db1a09cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching files found for SUN_NADIR.\n",
      "No matching files found for VELOCITY_NADIR.\n",
      "No matching files found for VELOCITY_SUN.\n",
      "No matching files found for VELOCITY_GS.\n",
      "No matching files found for SUN_VELOCITY.\n",
      "No matching files found for SUN_GS.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Input and output folders\n",
    "input_folder = \"5_operational_outputs\"\n",
    "output_folder = \"6_analysis_charts\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Attitude combinations to search for\n",
    "targets = [\"SUN_NADIR\", \"VELOCITY_NADIR\", \"VELOCITY_SUN\", \"VELOCITY_GS\", \"SUN_VELOCITY\", \"SUN_GS\"]\n",
    "\n",
    "for target in targets:\n",
    "    # Find all matching files for this target\n",
    "    file_pattern = os.path.join(input_folder, f\"min_*{target}*.csv\")\n",
    "    files = glob.glob(file_pattern)\n",
    "\n",
    "    if not files:\n",
    "        print(f\"No matching files found for {target}.\")\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for file in files:\n",
    "        try:\n",
    "            # Read CSV\n",
    "            df = pd.read_csv(file)\n",
    "            # Ensure the columns exist\n",
    "            if \"ELAPSED_TIME_SEC\" in df.columns and \"POWER_GENERATION_WATTS\" in df.columns:\n",
    "                # Normalize time so it starts at 0\n",
    "                time = df[\"ELAPSED_TIME_SEC\"] - df[\"ELAPSED_TIME_SEC\"].iloc[0]\n",
    "                power = df[\"POWER_GENERATION_WATTS\"]\n",
    "                plt.plot(time, power, label=os.path.basename(file))\n",
    "            else:\n",
    "                print(f\"Skipping {file}: Required columns not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    plt.xlabel(\"Elapsed Time (sec) [Normalized]\")\n",
    "    plt.ylabel(\"Power Generation (Watts)\")\n",
    "    plt.title(f\"Normalized Power Generation vs Time ({target} scenarios)\")\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot as a PNG\n",
    "    output_path = os.path.join(output_folder, f\"normalized_power_{target}.png\")\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved chart for {target} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d0714e",
   "metadata": {},
   "source": [
    "## What it does:\n",
    "Reads aggregated_power_orbit_table.csv.\n",
    "\n",
    "Extracts the Average Power per Orbit (row 4) and Sunlight Duration (row 1).\n",
    "\n",
    "Groups by Altitude and LTAN (ignoring attitude modes).\n",
    "\n",
    "Creates annotated heatmaps using Seaborn.\n",
    "\n",
    "Saves the heatmaps in 6_analysis_charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c13ea154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: 6_analysis_charts\\orbit_power_heatmap.png\n",
      "Saved: 6_analysis_charts\\orbit_sunlight_duration_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "# Generate heatmaps for average power and sunlight duration by orbit, altitude, and LTAN\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "\n",
    "# === CONFIG ===\n",
    "input_csv = \"aggregated_power_orbit_table.csv\"\n",
    "output_folder = \"6_analysis_charts\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# === LOAD DATA ===\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Helper function to process data and generate heatmap\n",
    "def generate_heatmap(row_index, title, cmap, filename, value_fmt=\".1f\"):\n",
    "    # Extract selected row\n",
    "    row = df.iloc[row_index, 1:]  # skip first column\n",
    "    columns = df.columns[1:]\n",
    "    \n",
    "    # Parse orbit & LTAN from column names\n",
    "    pattern = r'orbit_(\\d+)_ltan_(\\d+)_output'\n",
    "    orbits = []\n",
    "    for col in columns:\n",
    "        match = re.search(pattern, col)\n",
    "        if match:\n",
    "            orbits.append(f\"{match.group(1)} km / {match.group(2)}h\")\n",
    "        else:\n",
    "            orbits.append(col)\n",
    "\n",
    "    # Build dataframe\n",
    "    data = pd.DataFrame({'Orbit/LTAN': orbits, 'Value': row.values})\n",
    "    grouped = data.groupby('Orbit/LTAN').mean().reset_index()\n",
    "    grouped[['Altitude', 'LTAN']] = grouped['Orbit/LTAN'].str.extract(r'(\\d+) km / (\\d+)h')\n",
    "    pivot = grouped.pivot(index='Altitude', columns='LTAN', values='Value').astype(float)\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.heatmap(pivot, annot=True, fmt=value_fmt, cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Altitude (km)\")\n",
    "    plt.xlabel(\"LTAN (Hour)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and close\n",
    "    output_path = os.path.join(output_folder, filename)\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Saved: {output_path}\")\n",
    "\n",
    "# === Generate Heatmaps ===\n",
    "generate_heatmap(4, \"Average Power per Orbit [W] by Altitude and LTAN\", \"YlGnBu\", \"orbit_power_heatmap.png\")\n",
    "generate_heatmap(1, \"Sunlight Duration per Orbit [s] by Altitude and LTAN\", \"Oranges\", \"orbit_sunlight_duration_heatmap.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
