{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb91ac3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f79b3df",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: '--f=c:\\\\Users\\\\WilliamAvison\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v3c47f55ff71d6f46d6f9e7071bc96acda94032485.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 123\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    122\u001b[0m     csv_path \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sys\u001b[38;5;241m.\u001b[39margv) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data/x_down_info.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 123\u001b[0m     main(csv_path)\n",
      "Cell \u001b[1;32mIn[4], line 49\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(csv_path)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(csv_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m---> 49\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_path)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Build the \"library\" structure and a variable -> value map for the target column\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     lib \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\WilliamAvison\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\WilliamAvison\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\WilliamAvison\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\WilliamAvison\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\WilliamAvison\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: '--f=c:\\\\Users\\\\WilliamAvison\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v3c47f55ff71d6f46d6f9e7071bc96acda94032485.json'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Read the link-budget CSV, build a small 'library' (Parameter/python_variable/Units/Formula),\n",
    "and print results for the 'Endurosat X-band' column.\n",
    "\n",
    "Usage:\n",
    "    python print_endurosat_xband.py [/path/to/x_down_info.csv]\n",
    "\n",
    "If no path is given, defaults to /mnt/data/x_down_info.csv\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "TARGET_COLUMN = \"Endurosat X-band\"\n",
    "\n",
    "def to_float(x):\n",
    "    try:\n",
    "        # pandas may already give us float; if it's a string like \"1.38E-23\" this still works\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def guess_output_units(units_in, formula):\n",
    "    # If the formula is a log10 ratio (10*log10(...) or 20*log10(...)) we assume dB output\n",
    "    if isinstance(formula, str) and \"log10\" in formula:\n",
    "        return \"dB\"\n",
    "    return units_in if isinstance(units_in, str) else \"\"\n",
    "\n",
    "def fmt_num(x):\n",
    "    if x is None:\n",
    "        return \"N/A\"\n",
    "    # tidy numeric formatting\n",
    "    if abs(x) < 1e-3 or abs(x) >= 1e4:\n",
    "        return f\"{x:.3e}\"\n",
    "    # show up to 3 sig figs, strip trailing zeros\n",
    "    s = f\"{x:.3g}\"\n",
    "    # ensure something like \"3.00\" becomes \"3\"\n",
    "    try:\n",
    "        xi = float(s)\n",
    "        if abs(xi - round(xi)) < 1e-9:\n",
    "            return str(int(round(xi)))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return s\n",
    "\n",
    "def main(csv_path: str):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Build the \"library\" structure and a variable -> value map for the target column\n",
    "    lib = []\n",
    "    values = {}\n",
    "\n",
    "    required_cols = {\"Parameter\", \"python_variable\", \"Units\", \"Formula\", TARGET_COLUMN}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"CSV is missing required columns: {', '.join(sorted(missing))}\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        param = row[\"Parameter\"]\n",
    "        var   = row[\"python_variable\"]\n",
    "        units = row[\"Units\"]\n",
    "        formula = row[\"Formula\"]\n",
    "        raw_val = row[TARGET_COLUMN]\n",
    "\n",
    "        entry = {\n",
    "            \"Parameter\": param,\n",
    "            \"python_variable\": var,\n",
    "            \"Units\": units,\n",
    "            \"Formula\": formula if isinstance(formula, str) and formula.strip() else None,\n",
    "        }\n",
    "        lib.append(entry)\n",
    "\n",
    "        # seed values from the target column (numeric only)\n",
    "        val_f = to_float(raw_val)\n",
    "        if isinstance(var, str) and var and val_f is not None:\n",
    "            values[var] = val_f\n",
    "\n",
    "    # Evaluate formulas where present, using values (which include Endurosat X-band numbers)\n",
    "    # Put 'math' in the environment; keep builtins disabled.\n",
    "    env_globals = {\"__builtins__\": {}}\n",
    "    env_locals = {\"math\": math}\n",
    "    env_locals.update(values)  # make all known variables available\n",
    "\n",
    "    results = {}  # python_variable -> computed value (or raw if no formula)\n",
    "    for entry in lib:\n",
    "        var = entry[\"python_variable\"]\n",
    "        param = entry[\"Parameter\"]\n",
    "        units = entry[\"Units\"]\n",
    "        formula = entry[\"Formula\"]\n",
    "\n",
    "        # start from raw value if present\n",
    "        raw = values.get(var, None)\n",
    "\n",
    "        if formula:\n",
    "            try:\n",
    "                val = eval(formula, env_globals, env_locals)\n",
    "                # store and also update locals so downstream formulas can depend on earlier results\n",
    "                results[var] = val\n",
    "                env_locals[var] = val\n",
    "            except Exception:\n",
    "                # if the formula can't be evaluated, fall back to the raw value from the column\n",
    "                results[var] = raw\n",
    "        else:\n",
    "            results[var] = raw\n",
    "\n",
    "    # Print: \"Parameter: value [units]\"\n",
    "    for entry in lib:\n",
    "        param = entry[\"Parameter\"]\n",
    "        var   = entry[\"python_variable\"]\n",
    "        units_in = entry[\"Units\"]\n",
    "        formula  = entry[\"Formula\"]\n",
    "\n",
    "        val = results.get(var, None)\n",
    "        units_out = guess_output_units(units_in, formula)\n",
    "        units_str = f\" {units_out}\" if units_out else \"\"\n",
    "\n",
    "        print(f\"{param}: {fmt_num(val)}{units_str}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = sys.argv[1] if len(sys.argv) > 1 else \"/mnt/data/x_down_info.csv\"\n",
    "    main(csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f89186c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-Band lower limit (downlink): 8.03 GHz\n",
      "X-Band upper limit (downlink): 8.4 GHz\n",
      "Selected Carrier Frequency: 8.22 GHz\n",
      "Wavelength: nan m\n",
      "Tx Power: 3.01 dB\n",
      "Peak Boresight Antenna Gain: 17 dBi\n",
      "Interconnect losses (negative) / gains (positive): 0.5 dB\n",
      "Calculated EIRP: nan W\n",
      "Orbit altitude: 500 km\n",
      "Elevation ground to sat: 10 degrees\n",
      "Calculated slant range: 1690 km\n",
      "Free Space Path Loss term: -175 dB\n",
      "Atmospheric attenuation: 1 dB\n",
      "Rain attenuation ('fade'): 0.5 dB\n",
      "Scintillation loss: 1 dB\n",
      "Polarisation loss: 1 dB\n",
      "Other propagation losses: 2 dB\n",
      "Total Propagation Loss: -170 dB\n",
      "System symbol rate (or bandwidth): 30 MSps (or MHz)\n",
      "MODCOD scheme: N/A\n",
      "MODCOD implementation loss: 1.9 dB\n",
      "Spectral efficiency: 3 bps/Hz\n",
      "Other radio/protocol overheads: N/A (%)\n",
      "Channel Throughput: 89 Mbps\n",
      "CODMOD-inferred required Eb/N0 (inc. implementation losses): 7.39 dB\n",
      "Isotropically received power at satellite antenna input: nan dBW\n",
      "G/T: 24.4 dB/K\n",
      "Dish diameter: 3.7 m\n",
      "Calculated HPBW: 0.69 degrees\n",
      "Dish pointing error: 0.1 degrees\n",
      "Gain rolloff loss at pointing error: 0.28 dB\n",
      "Other overall link losses: 1.5 dB\n",
      "Boltzmann Constant: 1.380e-23 W/K/Hz\n",
      "Received Carrier-to-Noise ratio (bandwidth-independent): 89.4 dB-Hz\n",
      "Supported user data rate: 89 Mbps\n",
      "Data Rate in dB-Hz: 79.5 dB-Hz\n",
      "Received Eb/No: 9.92 dB\n",
      "Link Margin: 2.53 dB\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Read the link-budget CSV, build a small 'library' (Parameter/python_variable/Units/Formula),\n",
    "and print results for the 'Endurosat X-band' column.\n",
    "\n",
    "Usage (terminal):\n",
    "    python print_endurosat_xband.py /path/to/x_down_info.csv\n",
    "\n",
    "Usage (notebook):\n",
    "    # just run this cell; it will auto-pick a local CSV if available,\n",
    "    # otherwise fall back to /mnt/data/x_down_info.csv if present.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "TARGET_COLUMN = \"Endurosat X-band\"\n",
    "\n",
    "def to_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def guess_output_units(units_in, formula):\n",
    "    if isinstance(formula, str) and \"log10\" in formula:\n",
    "        return \"dB\"\n",
    "    return units_in if isinstance(units_in, str) else \"\"\n",
    "\n",
    "def fmt_num(x):\n",
    "    if x is None:\n",
    "        return \"N/A\"\n",
    "    if abs(x) < 1e-3 or abs(x) >= 1e4:\n",
    "        return f\"{x:.3e}\"\n",
    "    s = f\"{x:.3g}\"\n",
    "    try:\n",
    "        xi = float(s)\n",
    "        if abs(xi - round(xi)) < 1e-9:\n",
    "            return str(int(round(xi)))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return s\n",
    "\n",
    "def resolve_csv_path(cli_csv: str | None):\n",
    "    # If user provided a path and it exists, use it.\n",
    "    if cli_csv and cli_csv.lower().endswith(\".csv\") and os.path.isfile(cli_csv):\n",
    "        return cli_csv\n",
    "    # Otherwise try a few sensible defaults.\n",
    "    candidates = [\n",
    "        \"./x_down_info.csv\",\n",
    "        os.path.expanduser(\"~/Downloads/x_down_info.csv\"),\n",
    "        \"/mnt/data/x_down_info.csv\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if os.path.isfile(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def main(csv_path: str):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    required_cols = {\"Parameter\", \"python_variable\", \"Units\", \"Formula\", TARGET_COLUMN}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"CSV is missing required columns: {', '.join(sorted(missing))}\")\n",
    "\n",
    "    # Build the library + seed values from the target column\n",
    "    lib = []\n",
    "    values = {}\n",
    "    for _, row in df.iterrows():\n",
    "        entry = {\n",
    "            \"Parameter\": row[\"Parameter\"],\n",
    "            \"python_variable\": row[\"python_variable\"],\n",
    "            \"Units\": row[\"Units\"],\n",
    "            \"Formula\": row[\"Formula\"] if isinstance(row[\"Formula\"], str) and row[\"Formula\"].strip() else None,\n",
    "        }\n",
    "        lib.append(entry)\n",
    "\n",
    "        var = entry[\"python_variable\"]\n",
    "        raw_val = row[TARGET_COLUMN]\n",
    "        val_f = to_float(raw_val)\n",
    "        if isinstance(var, str) and var and val_f is not None:\n",
    "            values[var] = val_f\n",
    "\n",
    "    # Safe-ish eval env\n",
    "    env_globals = {\"__builtins__\": {}}\n",
    "    env_locals = {\"math\": math}\n",
    "    env_locals.update(values)\n",
    "\n",
    "    results = {}\n",
    "    for entry in lib:\n",
    "        var = entry[\"python_variable\"]\n",
    "        formula = entry[\"Formula\"]\n",
    "        raw = values.get(var, None)\n",
    "        if formula:\n",
    "            try:\n",
    "                val = eval(formula, env_globals, env_locals)\n",
    "                results[var] = val\n",
    "                env_locals[var] = val\n",
    "            except Exception:\n",
    "                results[var] = raw\n",
    "        else:\n",
    "            results[var] = raw\n",
    "\n",
    "    # Print \"Parameter: value units\"\n",
    "    for entry in lib:\n",
    "        param = entry[\"Parameter\"]\n",
    "        var = entry[\"python_variable\"]\n",
    "        units_in = entry[\"Units\"]\n",
    "        formula = entry[\"Formula\"]\n",
    "        val = results.get(var, None)\n",
    "        units_out = guess_output_units(units_in, formula)\n",
    "        units_str = f\" {units_out}\" if units_out else \"\"\n",
    "        print(f\"{param}: {fmt_num(val)}{units_str}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Robust argument parsing (ignores Jupyter's --f=... argument)\n",
    "    parser = argparse.ArgumentParser(add_help=True)\n",
    "    parser.add_argument(\"csv\", nargs=\"?\", help=\"Path to the CSV file\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    csv_path = resolve_csv_path(args.csv)\n",
    "\n",
    "    if not csv_path:\n",
    "        raise SystemExit(\n",
    "            \"Could not find a CSV. Pass a path explicitly, e.g.\\n\"\n",
    "            \"  python print_endurosat_xband.py C:\\\\path\\\\to\\\\x_down_info.csv\\n\"\n",
    "            \"or place x_down_info.csv in the current folder.\"\n",
    "        )\n",
    "\n",
    "    main(csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a86c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41313039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote CSV -> Endurosat_X-band_link_margin_vs_elevation.csv\n",
      "Wrote PNG -> Endurosat_X-band_link_margin_vs_elevation.png\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Compute Endurosat X-band link margin vs. elevation for two altitudes and\n",
    "export both a PNG plot and a CSV table.\n",
    "\n",
    "Inputs pulled from CSV (Endurosat X-band column):\n",
    "- selected_carrier_frequency [GHz]\n",
    "- tx_power [W]\n",
    "- peak_boresight_antenna_gain [dBi]\n",
    "- interconnect_losses_gains [dB]  (positive = loss)\n",
    "- atmospheric_attenuation, rain_attenuation, scintillation_loss,\n",
    "  polarisation_loss, other_propagation_losses [dB]\n",
    "- g_t [dB/K]\n",
    "- data_rate_in_db_hz [dB-Hz]\n",
    "- codmod_inferred_required_eb_n0 [dB]\n",
    "- other_overall_link_losses, gain_rolloff_loss_at_pointing_error [dB]\n",
    "\n",
    "Outputs:\n",
    "- ./Endurosat_X-band_link_margin_vs_elevation.csv\n",
    "- ./Endurosat_X-band_link_margin_vs_elevation.png\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EARTH_RADIUS_KM = 6371.0\n",
    "TARGET_COLUMN = \"Endurosat X-band\"\n",
    "\n",
    "# -------- helpers --------\n",
    "def to_float(x):\n",
    "    try:\n",
    "        return float(str(x).replace(\"%\", \"\").strip())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def resolve_csv_path(cli_csv: str | None):\n",
    "    if cli_csv and cli_csv.lower().endswith(\".csv\") and Path(cli_csv).is_file():\n",
    "        return cli_csv\n",
    "    for p in [\"./x_down_info.csv\",\n",
    "              os.path.expanduser(\"~/Downloads/x_down_info.csv\"),\n",
    "              \"/mnt/data/x_down_info.csv\"]:\n",
    "        if Path(p).is_file():\n",
    "            return p\n",
    "    raise FileNotFoundError(\n",
    "        \"CSV not found. Pass a path explicitly (e.g. --csv C:\\\\path\\\\x_down_info.csv) \"\n",
    "        \"or place x_down_info.csv in the current folder.\"\n",
    "    )\n",
    "\n",
    "def get_var(df: pd.DataFrame, varname: str, default=None):\n",
    "    s = df.loc[df[\"python_variable\"] == varname, TARGET_COLUMN]\n",
    "    return to_float(s.iloc[0]) if not s.empty else default\n",
    "\n",
    "def slant_range_km(alt_km: float, elev_deg: float) -> float:\n",
    "    e = math.radians(elev_deg)\n",
    "    R = EARTH_RADIUS_KM\n",
    "    H = R + alt_km\n",
    "    return -R * math.sin(e) + math.sqrt((R * math.sin(e)) ** 2 + H ** 2 - R ** 2)\n",
    "\n",
    "def fspl_db(range_km: float, freq_ghz: float) -> float:\n",
    "    # Standard: 92.45 + 20log10(R[km]) + 20log10(f[GHz])\n",
    "    return 92.45 + 20 * math.log10(range_km) + 20 * math.log10(freq_ghz)\n",
    "\n",
    "def eirp_dbw(P_w: float, G_tx_dbi: float, L_tx_db: float) -> float:\n",
    "    return 10 * math.log10(P_w) + (G_tx_dbi or 0.0) - (L_tx_db or 0.0)\n",
    "\n",
    "def cn0_dbhz(eirp_dbw: float, L_total_db: float, G_T_dbk: float,\n",
    "             k_val: float = 1.38e-23, extra_losses_db: float = 0.0) -> float:\n",
    "    k_db = 10 * math.log10(k_val)\n",
    "    return eirp_dbw - L_total_db + (G_T_dbk or 0.0) - k_db - (extra_losses_db or 0.0)\n",
    "\n",
    "# -------- main compute --------\n",
    "def compute_table(csv_path: str,\n",
    "                  altitudes=(500, 600),\n",
    "                  elevations=(0, 5, 10, 15, 20)) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # constants from CSV\n",
    "    f_ghz = get_var(df, \"selected_carrier_frequency\")\n",
    "    tx_power_w = get_var(df, \"tx_power\")\n",
    "    Gtx_dbi = get_var(df, \"peak_boresight_antenna_gain\")\n",
    "    Ltx_db = get_var(df, \"interconnect_losses_gains\", 0.0)\n",
    "\n",
    "    atm = get_var(df, \"atmospheric_attenuation\", 0.0)\n",
    "    rain = get_var(df, \"rain_attenuation\", 0.0)\n",
    "    scint = get_var(df, \"scintillation_loss\", 0.0)\n",
    "    pol = get_var(df, \"polarisation_loss\", 0.0)\n",
    "    other_prop = get_var(df, \"other_propagation_losses\", 0.0)\n",
    "\n",
    "    G_T = get_var(df, \"g_t\")\n",
    "    Rb_dBHz = get_var(df, \"data_rate_in_db_hz\")\n",
    "    EbN0_req = get_var(df, \"codmod_inferred_required_eb_n0\", 0.0)\n",
    "    other_overall = get_var(df, \"other_overall_link_losses\", 0.0)\n",
    "    rolloff = get_var(df, \"gain_rolloff_loss_at_pointing_error\", 0.0)\n",
    "\n",
    "    rows = []\n",
    "    for alt in altitudes:\n",
    "        for el in elevations:\n",
    "            sr_km = slant_range_km(alt, el)\n",
    "            Lfs = fspl_db(sr_km, f_ghz)\n",
    "            Ltotal = Lfs + atm + rain + scint + pol + other_prop\n",
    "            eirp = eirp_dbw(tx_power_w, Gtx_dbi, Ltx_db)\n",
    "            cn0 = cn0_dbhz(eirp, Ltotal, G_T, 1.38e-23, other_overall + rolloff)\n",
    "            ebn0 = cn0 - Rb_dBHz\n",
    "            margin = ebn0 - EbN0_req\n",
    "\n",
    "            rows.append({\n",
    "                \"orbit_altitude_km\": alt,\n",
    "                \"elevation_deg\": el,\n",
    "                \"slant_range_km\": sr_km,\n",
    "                \"fspl_db\": Lfs,\n",
    "                \"total_prop_loss_db\": Ltotal,\n",
    "                \"eirp_dbw\": eirp,\n",
    "                \"cn0_dbhz\": cn0,\n",
    "                \"ebn0_db\": ebn0,\n",
    "                \"link_margin_db\": margin,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def make_plot(df_out: pd.DataFrame, png_path: str):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for alt in sorted(df_out[\"orbit_altitude_km\"].unique()):\n",
    "        subset = df_out[df_out[\"orbit_altitude_km\"] == alt].sort_values(\"elevation_deg\")\n",
    "        plt.plot(subset[\"elevation_deg\"], subset[\"link_margin_db\"], marker=\"o\", label=f\"{int(alt)} km\")\n",
    "    plt.title(\"Link Margin vs Elevation for Endurosat X-band\")\n",
    "    plt.xlabel(\"Elevation (degrees)\")\n",
    "    plt.ylabel(\"Link Margin (dB)\")\n",
    "    plt.xticks(sorted(df_out[\"elevation_deg\"].unique()))\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(png_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Generate Link Margin vs Elevation plot and CSV.\")\n",
    "    parser.add_argument(\"--csv\", help=\"Path to x_down_info.csv\")\n",
    "    parser.add_argument(\"--out-csv\", default=\"Endurosat_X-band_link_margin_vs_elevation.csv\")\n",
    "    parser.add_argument(\"--out-png\", default=\"Endurosat_X-band_link_margin_vs_elevation.png\")\n",
    "    parser.add_argument(\"--alts\", nargs=\"*\", type=float, default=[500, 600],\n",
    "                        help=\"Altitudes in km (space-separated)\")\n",
    "    parser.add_argument(\"--elevs\", nargs=\"*\", type=float, default=[0, 5, 10, 15, 20],\n",
    "                        help=\"Elevations in degrees (space-separated)\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    csv_path = resolve_csv_path(args.csv)\n",
    "\n",
    "    df_out = compute_table(csv_path, altitudes=args.alts, elevations=args.elevs)\n",
    "    df_out.to_csv(args.out_csv, index=False)\n",
    "    make_plot(df_out, args.out_png)\n",
    "\n",
    "    print(f\"Wrote CSV -> {args.out_csv}\")\n",
    "    print(f\"Wrote PNG -> {args.out_png}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f962e476",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "226692ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500 km] time≥thr: 396.9s  data: 3725.6 MiB\n",
      "[600 km] time≥thr: 422.5s  data: 3181.9 MiB\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Notebook/CLI friendly pass data budget generator.\n",
    "\n",
    "- If run with CLI args, behaves like before.\n",
    "- If run with *no* args (e.g., pasted into a Jupyter cell), it:\n",
    "  * looks for Endurosat_X-band_link_margin_vs_elevation.csv and x_down_info.csv\n",
    "    in ./ or /mnt/data\n",
    "  * finds all altitudes in the link CSV\n",
    "  * writes pass_data_budget_<alt>km.csv for each altitude\n",
    "\"\"\"\n",
    "\n",
    "import argparse, math, sys, os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "MU_EARTH = 398600.4418  # km^3/s^2\n",
    "R_EARTH = 6371.0        # km\n",
    "\n",
    "def find_file(name):\n",
    "    for p in [Path(name), Path(\"/mnt/data\")/name]:\n",
    "        if p.is_file():\n",
    "            return str(p)\n",
    "    return None\n",
    "\n",
    "def get_supported_rate(info_csv: str):\n",
    "    df = pd.read_csv(info_csv)\n",
    "    s = df.loc[df[\"python_variable\"].astype(str)==\"supported_user_data_rate\"]\n",
    "    if s.empty:\n",
    "        s = df[df[\"Parameter\"].astype(str).str.contains(\"Supported user data rate\", case=False, na=False)]\n",
    "    if s.empty:\n",
    "        raise ValueError(\"Could not find 'Supported user data rate' in x_down_info.csv\")\n",
    "    val = s.iloc[0][\"Endurosat X-band\"]\n",
    "    units = s.iloc[0][\"Units\"] if \"Units\" in df.columns else \"\"\n",
    "    rate_val = float(str(val).strip())\n",
    "    unit_str = (units or \"\").lower()\n",
    "    if \"mb\" in unit_str:  bps, pretty = rate_val*1e6, \"Mb/s\"\n",
    "    elif \"kb\" in unit_str: bps, pretty = rate_val*1e3, \"kb/s\"\n",
    "    elif \"b/s\" in unit_str or \"bps\" in unit_str: bps, pretty = rate_val, \"b/s\"\n",
    "    else: bps, pretty = rate_val*1e6, \"Mb/s (assumed)\"\n",
    "    return bps, pretty\n",
    "\n",
    "def central_angle_from_slant(range_km: float, alt_km: float) -> float:\n",
    "    H = R_EARTH + alt_km\n",
    "    rho = range_km\n",
    "    cos_theta = (R_EARTH*R_EARTH + H*H - rho*rho) / (2*R_EARTH*H)\n",
    "    return math.acos(max(-1.0, min(1.0, cos_theta)))\n",
    "\n",
    "def mean_motion_rad_s(alt_km: float) -> float:\n",
    "    a = R_EARTH + alt_km\n",
    "    return math.sqrt(MU_EARTH / (a*a*a))\n",
    "\n",
    "def build_budget(link_csv: str, info_csv: str, alt_km: float,\n",
    "                 emin: float | None, margin_min: float, out_csv: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(link_csv)\n",
    "    need = {\"orbit_altitude_km\",\"elevation_deg\",\"link_margin_db\",\"slant_range_km\"}\n",
    "    if not need.issubset(df.columns):\n",
    "        raise ValueError(f\"link CSV missing columns: {need - set(df.columns)}\")\n",
    "    dfa = df[df[\"orbit_altitude_km\"].astype(float)==float(alt_km)].copy()\n",
    "    if dfa.empty:\n",
    "        raise ValueError(f\"No rows for orbit_altitude_km={alt_km}\")\n",
    "    dfa.sort_values(\"elevation_deg\", inplace=True)\n",
    "    bps, unit_str = get_supported_rate(info_csv)\n",
    "    if emin is None:\n",
    "        emin = float(dfa[\"elevation_deg\"].min())\n",
    "\n",
    "    e = dfa[\"elevation_deg\"].to_list()\n",
    "    m = dfa[\"link_margin_db\"].to_list()\n",
    "    r = dfa[\"slant_range_km\"].to_list()\n",
    "    theta = [central_angle_from_slant(x, alt_km) for x in r]\n",
    "    n = mean_motion_rad_s(alt_km)\n",
    "\n",
    "    rows = []\n",
    "    for i in range(len(e)-1):\n",
    "        e0, e1 = e[i], e[i+1]\n",
    "        m0, m1 = m[i], m[i+1]\n",
    "        th0, th1 = theta[i], theta[i+1]\n",
    "        if max(e0,e1) < emin: continue\n",
    "        e_start, e_end = max(min(e0,e1), emin), max(e0,e1)\n",
    "        if e_end <= e_start: continue\n",
    "\n",
    "        def lerp(x0,y0,x1,y1,x):\n",
    "            t=(x-x0)/(x1-x0); return y0+t*(y1-y0)\n",
    "\n",
    "        if e1 < e0:  # ensure ascending\n",
    "            e0,e1 = e1,e0; m0,m1 = m1,m0; th0,th1 = th1,th0\n",
    "\n",
    "        th_start = lerp(e0,th0,e1,th1,e_start)\n",
    "        th_end   = lerp(e0,th0,e1,th1,e_end)\n",
    "        m_start  = lerp(e0,m0, e1,m1, e_start)\n",
    "        m_end    = lerp(e0,m0, e1,m1, e_end)\n",
    "\n",
    "        dtheta = max(0.0, th_start - th_end)\n",
    "        bin_time_s = 2.0 * dtheta / n  # rise+set\n",
    "\n",
    "        usable_fraction = 1.0\n",
    "        if (m_start < margin_min) and (m_end < margin_min):\n",
    "            usable_fraction = 0.0\n",
    "        elif (m_start < margin_min) or (m_end < margin_min):\n",
    "            if m_end != m_start:\n",
    "                e_cross = e_start + (margin_min - m_start) * (e_end - e_start) / (m_end - m_start)\n",
    "                usable_fraction = (e_end - e_cross) / (e_end - e_start) if e_start <= e_cross <= e_end else (1.0 if m_end >= margin_min else 0.0)\n",
    "            else:\n",
    "                usable_fraction = 0.0 if m_start < margin_min else 1.0\n",
    "\n",
    "        data_bits = bps * bin_time_s * usable_fraction\n",
    "        data_mib  = data_bits / (1024*1024*8)\n",
    "\n",
    "        rows.append({\n",
    "            \"alt_km\": float(alt_km),\n",
    "            \"elev_start_deg\": float(e_start),\n",
    "            \"elev_end_deg\": float(e_end),\n",
    "            \"theta_start_deg\": math.degrees(th_start),\n",
    "            \"theta_end_deg\": math.degrees(th_end),\n",
    "            \"bin_time_s\": bin_time_s,\n",
    "            \"margin_start_db\": m_start,\n",
    "            \"margin_end_db\": m_end,\n",
    "            \"usable_fraction\": usable_fraction,\n",
    "            \"data_bits\": data_bits,\n",
    "            \"data_MiB\": data_mib,\n",
    "            \"rate_bps\": bps,\n",
    "            \"rate_unit\": unit_str,\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    out.to_csv(out_csv, index=False)\n",
    "    print(f\"[{int(alt_km)} km] time≥thr: {out['bin_time_s'].sum():.1f}s  data: {out['data_MiB'].sum():.1f} MiB\")\n",
    "    return out\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Create data budget for a pass from two CSVs.\")\n",
    "    parser.add_argument(\"--link-csv\", required=False, help=\"Path to Endurosat_X-band_link_margin_vs_elevation.csv\")\n",
    "    parser.add_argument(\"--info-csv\", required=False, help=\"Path to x_down_info.csv\")\n",
    "    parser.add_argument(\"--alt-km\", type=float, required=False, help=\"Orbit altitude to use\")\n",
    "    parser.add_argument(\"--emin\", type=float, default=None, help=\"Minimum elevation (deg)\")\n",
    "    parser.add_argument(\"--margin-min\", type=float, default=0.0, help=\"Minimum link margin counted (dB)\")\n",
    "    parser.add_argument(\"--out-csv\", required=False, help=\"Output CSV path\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # If no args given, auto mode (nice for notebooks)\n",
    "    if len(sys.argv) == 1 or (args.link_csv is None and args.info_csv is None):\n",
    "        link_csv = find_file(\"Endurosat_X-band_link_margin_vs_elevation.csv\")\n",
    "        info_csv = find_file(\"x_down_info.csv\")\n",
    "        if not link_csv or not info_csv:\n",
    "            raise SystemExit(\"Auto-mode couldn't find the CSVs. Put them next to the script or pass --link-csv/--info-csv.\")\n",
    "        df = pd.read_csv(link_csv)\n",
    "        alts = sorted(set(df[\"orbit_altitude_km\"]))\n",
    "        for alt in alts:\n",
    "            out_csv = f\"pass_data_budget_{int(alt)}km.csv\"\n",
    "            build_budget(link_csv, info_csv, float(alt), args.emin, args.margin_min, out_csv)\n",
    "        return\n",
    "\n",
    "    # CLI mode\n",
    "    if not all([args.link_csv, args.info_csv, args.alt_km, args.out_csv]):\n",
    "        parser.error(\"the following arguments are required: --link-csv, --info-csv, --alt-km, --out-csv\")\n",
    "\n",
    "    build_budget(args.link_csv, args.info_csv, args.alt_km, args.emin, args.margin_min, args.out_csv)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d08b74",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f882c3f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "c:\\Users\\WilliamAvison\\AppData\\Roaming\\jupyter\\runtime\\kernel-v3c47f55ff71d6f46d6f9e7071bc96acda94032485.json missing columns: {'alt_km', 'data_MiB', 'bin_time_s', 'elev_end_deg', 'elev_start_deg'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 151\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m -\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(Path(args\u001b[38;5;241m.\u001b[39moutdir)\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass_data_budget_all_cumulative.png\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 151\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[11], line 133\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    131\u001b[0m dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m--> 133\u001b[0m     df \u001b[38;5;241m=\u001b[39m load_pass_csv(f)\n\u001b[0;32m    134\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[0;32m    135\u001b[0m     out_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(Path(args\u001b[38;5;241m.\u001b[39moutdir)\u001b[38;5;241m/\u001b[39mPath(f)\u001b[38;5;241m.\u001b[39mwith_suffix(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mname)\n",
      "Cell \u001b[1;32mIn[11], line 45\u001b[0m, in \u001b[0;36mload_pass_csv\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     43\u001b[0m missing \u001b[38;5;241m=\u001b[39m required \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m missing columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Sort by elevation start\u001b[39;00m\n\u001b[0;32m     47\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melev_start_deg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melev_end_deg\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: c:\\Users\\WilliamAvison\\AppData\\Roaming\\jupyter\\runtime\\kernel-v3c47f55ff71d6f46d6f9e7071bc96acda94032485.json missing columns: {'alt_km', 'data_MiB', 'bin_time_s', 'elev_end_deg', 'elev_start_deg'}"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Plot pass data budget CSVs produced by make_pass_data_budget.py.\n",
    "\n",
    "Features\n",
    "- Auto-detects files named pass_data_budget_*km.csv in the working directory\n",
    "  (or accept explicit paths via --files).\n",
    "- For each CSV, creates two figures:\n",
    "    1) per-bin data contribution vs elevation (MiB per elevation bin)\n",
    "    2) cumulative data vs elevation (MiB)\n",
    "- Also creates combined comparison plots overlaying all inputs:\n",
    "    a) per-bin contributions\n",
    "    b) cumulative totals\n",
    "- Saves PNGs next to the inputs (or --outdir).\n",
    "\n",
    "Usage (auto-discover):\n",
    "    python plot_pass_data_budgets.py\n",
    "\n",
    "Usage (explicit files and output dir):\n",
    "    python plot_pass_data_budgets.py --files pass_data_budget_500km.csv pass_data_budget_600km.csv --outdir plots/\n",
    "\n",
    "Notes\n",
    "- Assumes CSV has columns from make_pass_data_budget.py:\n",
    "  elev_start_deg, elev_end_deg, data_MiB, alt_km, bin_time_s\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RE_ALT = re.compile(r'(\\d+)\\s*km', re.IGNORECASE)\n",
    "\n",
    "def parse_alt_from_filename(path: str):\n",
    "    m = RE_ALT.search(Path(path).name)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def load_pass_csv(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    required = {\"elev_start_deg\",\"elev_end_deg\",\"data_MiB\",\"alt_km\",\"bin_time_s\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"{path} missing columns: {missing}\")\n",
    "    # Sort by elevation start\n",
    "    df = df.sort_values([\"elev_start_deg\",\"elev_end_deg\"]).reset_index(drop=True)\n",
    "    # Add midpoint elevation and cumulative data\n",
    "    df[\"elev_mid_deg\"] = 0.5*(df[\"elev_start_deg\"] + df[\"elev_end_deg\"])\n",
    "    df[\"cum_MiB\"] = df[\"data_MiB\"].cumsum()\n",
    "    # Altitude column might be float; keep an int label for display\n",
    "    alt_from_file = parse_alt_from_filename(path)\n",
    "    alt_from_col = None if df.empty else int(round(df[\"alt_km\"].iloc[0]))\n",
    "    df[\"alt_label\"] = alt_from_file if alt_from_file is not None else alt_from_col\n",
    "    return df\n",
    "\n",
    "def find_default_files():\n",
    "    here = Path(\".\")\n",
    "    candidates = list(here.glob(\"pass_data_budget_*km.csv\"))\n",
    "    if not candidates:\n",
    "        mnt = Path(\"/mnt/data\")\n",
    "        candidates = list(mnt.glob(\"pass_data_budget_*km.csv\"))\n",
    "    return [str(p) for p in candidates]\n",
    "\n",
    "def plot_per_file(df: pd.DataFrame, out_png_base: str):\n",
    "    # Per-bin contribution\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(df[\"elev_mid_deg\"], df[\"data_MiB\"], marker=\"o\")\n",
    "    plt.title(f\"Per-bin Data vs Elevation ({int(df['alt_km'].iloc[0])} km)\")\n",
    "    plt.xlabel(\"Elevation (degrees)\")\n",
    "    plt.ylabel(\"Data per bin (MiB)\")\n",
    "    plt.grid(True)\n",
    "    plt.xticks(sorted(set(df[\"elev_start_deg\"]).union(set(df[\"elev_end_deg\"]))))\n",
    "    plt.savefig(out_png_base + \"_per_bin.png\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Cumulative\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(df[\"elev_end_deg\"], df[\"cum_MiB\"], marker=\"o\")\n",
    "    plt.title(f\"Cumulative Data vs Elevation ({int(df['alt_km'].iloc[0])} km)\")\n",
    "    plt.xlabel(\"Elevation (degrees)\")\n",
    "    plt.ylabel(\"Cumulative Data (MiB)\")\n",
    "    plt.grid(True)\n",
    "    plt.xticks(sorted(set(df[\"elev_start_deg\"]).union(set(df[\"elev_end_deg\"]))))\n",
    "    plt.savefig(out_png_base + \"_cumulative.png\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_combined(dfs: list[pd.DataFrame], outdir: str):\n",
    "    # Combined per-bin\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for df in dfs:\n",
    "        label = f\"{int(df['alt_km'].iloc[0])} km\"\n",
    "        plt.plot(df[\"elev_mid_deg\"], df[\"data_MiB\"], marker=\"o\", label=label)\n",
    "    plt.title(\"Per-bin Data vs Elevation (All Altitudes)\")\n",
    "    plt.xlabel(\"Elevation (degrees)\")\n",
    "    plt.ylabel(\"Data per bin (MiB)\")\n",
    "    plt.grid(True)\n",
    "    # Use union of ticks across all\n",
    "    ticks = sorted(set().union(*[set(df[\"elev_start_deg\"]).union(set(df[\"elev_end_deg\"])) for df in dfs]))\n",
    "    plt.xticks(ticks)\n",
    "    plt.legend()\n",
    "    plt.savefig(str(Path(outdir)/\"pass_data_budget_all_per_bin.png\"), bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Combined cumulative\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for df in dfs:\n",
    "        label = f\"{int(df['alt_km'].iloc[0])} km\"\n",
    "        plt.plot(df[\"elev_end_deg\"], df[\"cum_MiB\"], marker=\"o\", label=label)\n",
    "    plt.title(\"Cumulative Data vs Elevation (All Altitudes)\")\n",
    "    plt.xlabel(\"Elevation (degrees)\")\n",
    "    plt.ylabel(\"Cumulative Data (MiB)\")\n",
    "    plt.grid(True)\n",
    "    ticks = sorted(set().union(*[set(df[\"elev_start_deg\"]).union(set(df[\"elev_end_deg\"])) for df in dfs]))\n",
    "    plt.xticks(ticks)\n",
    "    plt.legend()\n",
    "    plt.savefig(str(Path(outdir)/\"pass_data_budget_all_cumulative.png\"), bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser(description=\"Plot pass data budget CSVs.\")\n",
    "    ap.add_argument(\"--files\", nargs=\"*\", help=\"Specific CSV files to plot.\")\n",
    "    ap.add_argument(\"--outdir\", default=\".\", help=\"Output directory for PNGs.\")\n",
    "    args, _ = ap.parse_known_args()\n",
    "\n",
    "    files = args.files if args.files else find_default_files()\n",
    "    if not files:\n",
    "        raise SystemExit(\"No pass_data_budget_*km.csv files found. Provide --files or run make_pass_data_budget.py first.\")\n",
    "    os.makedirs(args.outdir, exist_ok=True)\n",
    "\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        df = load_pass_csv(f)\n",
    "        dfs.append(df)\n",
    "        out_base = str(Path(args.outdir)/Path(f).with_suffix(\"\").name)\n",
    "        plot_per_file(df, out_base)\n",
    "\n",
    "    if len(dfs) > 1:\n",
    "        plot_combined(dfs, args.outdir)\n",
    "\n",
    "    print(f\"Wrote plots to: {args.outdir}\")\n",
    "    for f in files:\n",
    "        base = str(Path(args.outdir)/Path(f).with_suffix(\"\").name)\n",
    "        print(\" -\", base + \"_per_bin.png\")\n",
    "        print(\" -\", base + \"_cumulative.png\")\n",
    "    if len(dfs) > 1:\n",
    "        print(\" -\", str(Path(args.outdir)/\"pass_data_budget_all_per_bin.png\"))\n",
    "        print(\" -\", str(Path(args.outdir)/\"pass_data_budget_all_cumulative.png\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311aa1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43846de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500 km] time≥thr: 396.9s  data: 31252.3 Mb\n",
      "[600 km] time≥thr: 422.5s  data: 26691.8 Mb\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Notebook/CLI friendly pass data budget generator.\n",
    "\n",
    "- If run with CLI args, behaves like before.\n",
    "- If run with *no* args (e.g., pasted into a Jupyter cell), it:\n",
    "  * looks for Endurosat_X-band_link_margin_vs_elevation.csv and x_down_info.csv\n",
    "    in ./ or /mnt/data\n",
    "  * finds all altitudes in the link CSV\n",
    "  * writes pass_data_budget_<alt>km.csv for each altitude\n",
    "\n",
    "Outputs now include:\n",
    "- data_Mb  (megabits, decimal, Mb = 1e6 bits)\n",
    "- data_MiB (mebibytes, binary, MiB = 1024*1024 bytes)  # kept for compatibility\n",
    "\"\"\"\n",
    "\n",
    "import argparse, math, sys, os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "MU_EARTH = 398600.4418  # km^3/s^2\n",
    "R_EARTH = 6371.0        # km\n",
    "\n",
    "def find_file(name):\n",
    "    for p in [Path(name), Path(\"/mnt/data\")/name]:\n",
    "        if p.is_file():\n",
    "            return str(p)\n",
    "    return None\n",
    "\n",
    "def get_supported_rate(info_csv: str):\n",
    "    df = pd.read_csv(info_csv)\n",
    "    s = df.loc[df[\"python_variable\"].astype(str)==\"supported_user_data_rate\"]\n",
    "    if s.empty:\n",
    "        s = df[df[\"Parameter\"].astype(str).str.contains(\"Supported user data rate\", case=False, na=False)]\n",
    "    if s.empty:\n",
    "        raise ValueError(\"Could not find 'Supported user data rate' in x_down_info.csv\")\n",
    "    val = s.iloc[0][\"Endurosat X-band\"]\n",
    "    units = s.iloc[0][\"Units\"] if \"Units\" in df.columns else \"\"\n",
    "    rate_val = float(str(val).strip())\n",
    "    unit_str = (units or \"\").lower()\n",
    "    if \"mb\" in unit_str:  bps, pretty = rate_val*1e6, \"Mb/s\"\n",
    "    elif \"kb\" in unit_str: bps, pretty = rate_val*1e3, \"kb/s\"\n",
    "    elif \"b/s\" in unit_str or \"bps\" in unit_str: bps, pretty = rate_val, \"b/s\"\n",
    "    else: bps, pretty = rate_val*1e6, \"Mb/s (assumed)\"\n",
    "    return bps, pretty\n",
    "\n",
    "def central_angle_from_slant(range_km: float, alt_km: float) -> float:\n",
    "    H = R_EARTH + alt_km\n",
    "    rho = range_km\n",
    "    cos_theta = (R_EARTH*R_EARTH + H*H - rho*rho) / (2*R_EARTH*H)\n",
    "    return math.acos(max(-1.0, min(1.0, cos_theta)))\n",
    "\n",
    "def mean_motion_rad_s(alt_km: float) -> float:\n",
    "    a = R_EARTH + alt_km\n",
    "    return math.sqrt(MU_EARTH / (a*a*a))\n",
    "\n",
    "def build_budget(link_csv: str, info_csv: str, alt_km: float,\n",
    "                 emin: float | None, margin_min: float, out_csv: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(link_csv)\n",
    "    need = {\"orbit_altitude_km\",\"elevation_deg\",\"link_margin_db\",\"slant_range_km\"}\n",
    "    if not need.issubset(df.columns):\n",
    "        raise ValueError(f\"link CSV missing columns: {need - set(df.columns)}\")\n",
    "    dfa = df[df[\"orbit_altitude_km\"].astype(float)==float(alt_km)].copy()\n",
    "    if dfa.empty:\n",
    "        raise ValueError(f\"No rows for orbit_altitude_km={alt_km}\")\n",
    "    dfa.sort_values(\"elevation_deg\", inplace=True)\n",
    "    bps, unit_str = get_supported_rate(info_csv)\n",
    "    if emin is None:\n",
    "        emin = float(dfa[\"elevation_deg\"].min())\n",
    "\n",
    "    e = dfa[\"elevation_deg\"].to_list()\n",
    "    m = dfa[\"link_margin_db\"].to_list()\n",
    "    r = dfa[\"slant_range_km\"].to_list()\n",
    "    theta = [central_angle_from_slant(x, alt_km) for x in r]\n",
    "    n = mean_motion_rad_s(alt_km)\n",
    "\n",
    "    rows = []\n",
    "    for i in range(len(e)-1):\n",
    "        e0, e1 = e[i], e[i+1]\n",
    "        m0, m1 = m[i], m[i+1]\n",
    "        th0, th1 = theta[i], theta[i+1]\n",
    "        if max(e0,e1) < emin: continue\n",
    "        e_start, e_end = max(min(e0,e1), emin), max(e0,e1)\n",
    "        if e_end <= e_start: continue\n",
    "\n",
    "        def lerp(x0,y0,x1,y1,x):\n",
    "            t=(x-x0)/(x1-x0); return y0+t*(y1-y0)\n",
    "\n",
    "        if e1 < e0:  # ensure ascending\n",
    "            e0,e1 = e1,e0; m0,m1 = m1,m0; th0,th1 = th1,th0\n",
    "\n",
    "        th_start = lerp(e0,th0,e1,th1,e_start)\n",
    "        th_end   = lerp(e0,th0,e1,th1,e_end)\n",
    "        m_start  = lerp(e0,m0, e1,m1, e_start)\n",
    "        m_end    = lerp(e0,m0, e1,m1, e_end)\n",
    "\n",
    "        dtheta = max(0.0, th_start - th_end)\n",
    "        bin_time_s = 2.0 * dtheta / n  # rise+set\n",
    "\n",
    "        usable_fraction = 1.0\n",
    "        if (m_start < margin_min) and (m_end < margin_min):\n",
    "            usable_fraction = 0.0\n",
    "        elif (m_start < margin_min) or (m_end < margin_min):\n",
    "            if m_end != m_start:\n",
    "                e_cross = e_start + (margin_min - m_start) * (e_end - e_start) / (m_end - m_start)\n",
    "                usable_fraction = (e_end - e_cross) / (e_end - e_start) if e_start <= e_cross <= e_end else (1.0 if m_end >= margin_min else 0.0)\n",
    "            else:\n",
    "                usable_fraction = 0.0 if m_start < margin_min else 1.0\n",
    "\n",
    "        data_bits = bps * bin_time_s * usable_fraction\n",
    "        data_Mb   = data_bits / 1e6                 # decimal megabits\n",
    "        data_MiB  = data_bits / (1024*1024*8)       # binary mebibytes (kept for compatibility)\n",
    "\n",
    "        rows.append({\n",
    "            \"alt_km\": float(alt_km),\n",
    "            \"elev_start_deg\": float(e_start),\n",
    "            \"elev_end_deg\": float(e_end),\n",
    "            \"theta_start_deg\": math.degrees(th_start),\n",
    "            \"theta_end_deg\": math.degrees(th_end),\n",
    "            \"bin_time_s\": bin_time_s,\n",
    "            \"margin_start_db\": m_start,\n",
    "            \"margin_end_db\": m_end,\n",
    "            \"usable_fraction\": usable_fraction,\n",
    "            \"data_bits\": data_bits,\n",
    "            \"data_Mb\": data_Mb,     # NEW: megabits\n",
    "            \"data_MiB\": data_MiB,   # legacy column (optional for plots using MiB)\n",
    "            \"rate_bps\": bps,\n",
    "            \"rate_unit\": unit_str,\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    out.to_csv(out_csv, index=False)\n",
    "    print(f\"[{int(alt_km)} km] time≥thr: {out['bin_time_s'].sum():.1f}s  data: {out['data_Mb'].sum():.1f} Mb\")\n",
    "    return out\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Create data budget for a pass from two CSVs.\")\n",
    "    parser.add_argument(\"--link-csv\", required=False, help=\"Path to Endurosat_X-band_link_margin_vs_elevation.csv\")\n",
    "    parser.add_argument(\"--info-csv\", required=False, help=\"Path to x_down_info.csv\")\n",
    "    parser.add_argument(\"--alt-km\", type=float, required=False, help=\"Orbit altitude to use\")\n",
    "    parser.add_argument(\"--emin\", type=float, default=None, help=\"Minimum elevation (deg)\")\n",
    "    parser.add_argument(\"--margin-min\", type=float, default=0.0, help=\"Minimum link margin counted (dB)\")\n",
    "    parser.add_argument(\"--out-csv\", required=False, help=\"Output CSV path\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # If no args given, auto mode (nice for notebooks)\n",
    "    if len(sys.argv) == 1 or (args.link_csv is None and args.info_csv is None):\n",
    "        link_csv = find_file(\"Endurosat_X-band_link_margin_vs_elevation.csv\")\n",
    "        info_csv = find_file(\"x_down_info.csv\")\n",
    "        if not link_csv or not info_csv:\n",
    "            raise SystemExit(\"Auto-mode couldn't find the CSVs. Put them next to the script or pass --link-csv/--info-csv.\")\n",
    "        df = pd.read_csv(link_csv)\n",
    "        alts = sorted(set(df[\"orbit_altitude_km\"]))\n",
    "        for alt in alts:\n",
    "            out_csv = f\"pass_data_budget_{int(alt)}km.csv\"\n",
    "            build_budget(link_csv, info_csv, float(alt), args.emin, args.margin_min, out_csv)\n",
    "        return\n",
    "\n",
    "    # CLI mode\n",
    "    if not all([args.link_csv, args.info_csv, args.alt_km, args.out_csv]):\n",
    "        parser.error(\"the following arguments are required: --link-csv, --info-csv, --alt-km, --out-csv\")\n",
    "\n",
    "    build_budget(args.link_csv, args.info_csv, args.alt_km, args.emin, args.margin_min, args.out_csv)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fdb6b4",
   "metadata": {},
   "source": [
    "scale to 600s pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9559da0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500 km] base 396.9s → scaled 600.0s | data 47243.9 Mb → pass_data_budget_500km_scaled_10min.csv\n",
      "[600 km] base 422.5s → scaled 600.0s | data 37905.6 Mb → pass_data_budget_600km_scaled_10min.csv\n",
      "Wrote cumulative plot → pass_data_budget_scaled_10min_cumulative.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>alt_km</th>\n",
       "      <th>base_time_s</th>\n",
       "      <th>scaled_time_s</th>\n",
       "      <th>scaled_Mb</th>\n",
       "      <th>scale_factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pass_data_budget_500km.csv</td>\n",
       "      <td>500</td>\n",
       "      <td>396.905248</td>\n",
       "      <td>600.0</td>\n",
       "      <td>47243.936120</td>\n",
       "      <td>1.511696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pass_data_budget_600km.csv</td>\n",
       "      <td>600</td>\n",
       "      <td>422.499347</td>\n",
       "      <td>600.0</td>\n",
       "      <td>37905.572941</td>\n",
       "      <td>1.420121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         file  alt_km  base_time_s  scaled_time_s  \\\n",
       "0  pass_data_budget_500km.csv     500   396.905248          600.0   \n",
       "1  pass_data_budget_600km.csv     600   422.499347          600.0   \n",
       "\n",
       "      scaled_Mb  scale_factor  \n",
       "0  47243.936120      1.511696  \n",
       "1  37905.572941      1.420121  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notebook-friendly: scale pass_data_budget_*km.csv to a target duration\n",
    "# and produce scaled CSVs + a combined cumulative PNG.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Config you can tweak ---\n",
    "TARGET_DURATION_S = 600.0   # 10 minutes\n",
    "OUTDIR = Path(\".\")          # where to save outputs\n",
    "FILES = None                # e.g. [\"pass_data_budget_500km.csv\", \"pass_data_budget_600km.csv\"]; or leave None to auto-find\n",
    "# ----------------------------\n",
    "\n",
    "def find_pass_files():\n",
    "    pats = [Path(\".\").glob(\"pass_data_budget_*km.csv\"),\n",
    "            Path(\"/mnt/data\").glob(\"pass_data_budget_*km.csv\")]\n",
    "    files = []\n",
    "    for g in pats:\n",
    "        for p in g:\n",
    "            # guard: only CSVs with our naming pattern\n",
    "            if p.is_file() and p.suffix.lower() == \".csv\" and \"pass_data_budget_\" in p.name:\n",
    "                files.append(p.resolve())\n",
    "    # de-dup while preserving order\n",
    "    seen, unique = set(), []\n",
    "    for p in files:\n",
    "        if p not in seen:\n",
    "            unique.append(p); seen.add(p)\n",
    "    return unique\n",
    "\n",
    "def scale_one(path: Path, duration_s: float):\n",
    "    df = pd.read_csv(path)\n",
    "    required = {\"bin_time_s\", \"rate_bps\", \"usable_fraction\", \"alt_km\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        print(f\"⚠️  Skipping {path.name}: missing columns {missing}\")\n",
    "        return None, None\n",
    "\n",
    "    base_time = float(df[\"bin_time_s\"].sum())\n",
    "    if base_time <= 0:\n",
    "        print(f\"⚠️  Skipping {path.name}: total bin_time_s is zero\")\n",
    "        return None, None\n",
    "\n",
    "    scale = duration_s / base_time\n",
    "    d = df.copy()\n",
    "    d[\"bin_time_s_scaled\"] = d[\"bin_time_s\"] * scale\n",
    "    d[\"data_bits_scaled\"] = d[\"rate_bps\"] * d[\"bin_time_s_scaled\"] * d[\"usable_fraction\"]\n",
    "    d[\"data_Mb_scaled\"] = d[\"data_bits_scaled\"] / 1e6            # decimal megabits\n",
    "    d[\"data_MiB_scaled\"] = d[\"data_bits_scaled\"] / (1024**2 * 8) # binary mebibytes (optional)\n",
    "    d[\"cum_time_s_scaled\"] = d[\"bin_time_s_scaled\"].cumsum()\n",
    "    d[\"cum_Mb_scaled\"] = d[\"data_Mb_scaled\"].cumsum()\n",
    "    alt = int(round(d[\"alt_km\"].iloc[0]))\n",
    "    summary = {\n",
    "        \"file\": path.name,\n",
    "        \"alt_km\": alt,\n",
    "        \"base_time_s\": base_time,\n",
    "        \"scaled_time_s\": float(d[\"bin_time_s_scaled\"].sum()),\n",
    "        \"scaled_Mb\": float(d[\"data_Mb_scaled\"].sum()),\n",
    "        \"scale_factor\": scale,\n",
    "    }\n",
    "    return d, summary\n",
    "\n",
    "# Discover files\n",
    "paths = [Path(p) for p in FILES] if FILES else find_pass_files()\n",
    "if not paths:\n",
    "    raise SystemExit(\"No pass_data_budget_*km.csv files found. Run your budget generator first.\")\n",
    "\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "scaled_list, summaries = [], []\n",
    "for p in paths:\n",
    "    d, s = scale_one(p, TARGET_DURATION_S)\n",
    "    if d is None: \n",
    "        continue\n",
    "    minutes = int(round(TARGET_DURATION_S / 60))\n",
    "    alt = s[\"alt_km\"]\n",
    "    out_csv = OUTDIR / f\"pass_data_budget_{alt}km_scaled_{minutes}min.csv\"\n",
    "    d.to_csv(out_csv, index=False)\n",
    "    scaled_list.append(d)\n",
    "    summaries.append(s)\n",
    "    print(f\"[{alt} km] base {s['base_time_s']:.1f}s → scaled {s['scaled_time_s']:.1f}s | \"\n",
    "          f\"data {s['scaled_Mb']:.1f} Mb → {out_csv.name}\")\n",
    "\n",
    "# Plot combined cumulative Mb vs time\n",
    "if scaled_list:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for d in scaled_list:\n",
    "        alt = int(round(d[\"alt_km\"].iloc[0]))\n",
    "        plt.plot(d[\"cum_time_s_scaled\"], d[\"cum_Mb_scaled\"], marker=\"o\", label=f\"{alt} km\")\n",
    "    plt.title(f\"Cumulative Data vs Time (Scaled to {int(round(TARGET_DURATION_S/60))}-minute Pass)\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Cumulative Data (Mb)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    png_path = OUTDIR / f\"pass_data_budget_scaled_{int(round(TARGET_DURATION_S/60))}min_cumulative.png\"\n",
    "    plt.savefig(png_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Wrote cumulative plot → {png_path}\")\n",
    "\n",
    "# Show a quick summary table in the notebook (optional)\n",
    "pd.DataFrame(summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fff39a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500 km] base 396.9s → scaled 600.0s | data 5905.5 MB → pass_data_budget_500km_scaled_10min.csv\n",
      "[600 km] base 422.5s → scaled 600.0s | data 4738.2 MB → pass_data_budget_600km_scaled_10min.csv\n",
      "Wrote cumulative plot → pass_data_budget_scaled_10min_cumulative.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>alt_km</th>\n",
       "      <th>base_time_s</th>\n",
       "      <th>scaled_time_s</th>\n",
       "      <th>scaled_MB</th>\n",
       "      <th>scale_factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pass_data_budget_500km.csv</td>\n",
       "      <td>500</td>\n",
       "      <td>396.905248</td>\n",
       "      <td>600.0</td>\n",
       "      <td>5905.492015</td>\n",
       "      <td>1.511696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pass_data_budget_600km.csv</td>\n",
       "      <td>600</td>\n",
       "      <td>422.499347</td>\n",
       "      <td>600.0</td>\n",
       "      <td>4738.196618</td>\n",
       "      <td>1.420121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         file  alt_km  base_time_s  scaled_time_s  \\\n",
       "0  pass_data_budget_500km.csv     500   396.905248          600.0   \n",
       "1  pass_data_budget_600km.csv     600   422.499347          600.0   \n",
       "\n",
       "     scaled_MB  scale_factor  \n",
       "0  5905.492015      1.511696  \n",
       "1  4738.196618      1.420121  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Config ---\n",
    "TARGET_DURATION_S = 600.0   # 10 minutes\n",
    "OUTDIR = Path(\".\")\n",
    "FILES = None                # or like [\"pass_data_budget_500km.csv\", ...]\n",
    "# ---------------\n",
    "\n",
    "def find_pass_files():\n",
    "    pats = [Path(\".\").glob(\"pass_data_budget_*km.csv\"),\n",
    "            Path(\"/mnt/data\").glob(\"pass_data_budget_*km.csv\")]\n",
    "    files = []\n",
    "    for g in pats:\n",
    "        for p in g:\n",
    "            if p.is_file() and p.suffix.lower()==\".csv\" and \"pass_data_budget_\" in p.name:\n",
    "                files.append(p.resolve())\n",
    "    seen, uniq = set(), []\n",
    "    for p in files:\n",
    "        if p not in seen:\n",
    "            uniq.append(p); seen.add(p)\n",
    "    return uniq\n",
    "\n",
    "def scale_one(path: Path, duration_s: float):\n",
    "    df = pd.read_csv(path)\n",
    "    required = {\"bin_time_s\", \"rate_bps\", \"usable_fraction\", \"alt_km\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        print(f\"⚠️  Skipping {path.name}: missing {missing}\")\n",
    "        return None, None\n",
    "\n",
    "    base_time = float(df[\"bin_time_s\"].sum())\n",
    "    if base_time <= 0:\n",
    "        print(f\"⚠️  Skipping {path.name}: total bin_time_s is zero\")\n",
    "        return None, None\n",
    "\n",
    "    scale = duration_s / base_time\n",
    "    d = df.copy()\n",
    "    d[\"bin_time_s_scaled\"] = d[\"bin_time_s\"] * scale\n",
    "    d[\"data_bits_scaled\"]  = d[\"rate_bps\"] * d[\"bin_time_s_scaled\"] * d[\"usable_fraction\"]\n",
    "\n",
    "    # Megabytes (decimal) and Mebibytes (binary)\n",
    "    d[\"data_MB_scaled\"]  = d[\"data_bits_scaled\"] / (8 * 1e6)\n",
    "    d[\"data_MiB_scaled\"] = d[\"data_bits_scaled\"] / (8 * 1024**2)\n",
    "\n",
    "    # Cumulative for plotting\n",
    "    d[\"cum_time_s_scaled\"] = d[\"bin_time_s_scaled\"].cumsum()\n",
    "    d[\"cum_MB_scaled\"]     = d[\"data_MB_scaled\"].cumsum()\n",
    "\n",
    "    alt = int(round(d[\"alt_km\"].iloc[0]))\n",
    "    summary = {\n",
    "        \"file\": path.name,\n",
    "        \"alt_km\": alt,\n",
    "        \"base_time_s\": base_time,\n",
    "        \"scaled_time_s\": float(d[\"bin_time_s_scaled\"].sum()),\n",
    "        \"scaled_MB\": float(d[\"data_MB_scaled\"].sum()),\n",
    "        \"scale_factor\": scale,\n",
    "    }\n",
    "    return d, summary\n",
    "\n",
    "paths = [Path(p) for p in FILES] if FILES else find_pass_files()\n",
    "if not paths:\n",
    "    raise SystemExit(\"No pass_data_budget_*km.csv files found.\")\n",
    "\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "scaled_list, summaries = [], []\n",
    "for p in paths:\n",
    "    d, s = scale_one(p, TARGET_DURATION_S)\n",
    "    if d is None: \n",
    "        continue\n",
    "    minutes = int(round(TARGET_DURATION_S/60))\n",
    "    alt = s[\"alt_km\"]\n",
    "    out_csv = OUTDIR / f\"pass_data_budget_{alt}km_scaled_{minutes}min.csv\"\n",
    "    d.to_csv(out_csv, index=False)\n",
    "    scaled_list.append(d)\n",
    "    summaries.append(s)\n",
    "    print(f\"[{alt} km] base {s['base_time_s']:.1f}s → scaled {s['scaled_time_s']:.1f}s | \"\n",
    "          f\"data {s['scaled_MB']:.1f} MB → {out_csv.name}\")\n",
    "\n",
    "# Combined cumulative MB vs time\n",
    "if scaled_list:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for d in scaled_list:\n",
    "        alt = int(round(d[\"alt_km\"].iloc[0]))\n",
    "        plt.plot(d[\"cum_time_s_scaled\"], d[\"cum_MB_scaled\"], marker=\"o\", label=f\"{alt} km\")\n",
    "    plt.title(f\"Cumulative Data vs Time (Scaled to {int(round(TARGET_DURATION_S/60))}-minute Pass)\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Cumulative Data (MB)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    png_path = OUTDIR / f\"pass_data_budget_scaled_{int(round(TARGET_DURATION_S/60))}min_cumulative.png\"\n",
    "    plt.savefig(png_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Wrote cumulative plot → {png_path}\")\n",
    "\n",
    "pd.DataFrame(summaries)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
